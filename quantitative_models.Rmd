# Quantitative Models of Linguistic Data
aaa

## Table of Contents

* [Probability Distributions](#probability-distributions)
* [Fitting distributions](#fitting)
* [Regression Models](#regression-models)
* [Next steps](#further-readings)

## <a name="probability-distributions"></a>Probability Distributions

R supports multiple aspects of Probability Distributions for both descrete and continuous
random variables.

There are a lot of useful distributions in the `stats` package which is loaded by default in every `R` session.

```{r, eval = F}
?distributions
```


```{r, eval = F}
library(ggplot2)

nd_values <- rnorm(100, 0, 2)

head(nd_values)

qplot(nd_values, geom = c('histogram', 'density'), binwidth = 0.05)

```

### Binomial distribution

$$
P(X = x) = \binom{n}{x}\cdot p^x\cdot q^{n-x}
$$

`rbinom(n, size, prob)` - we toss _n_ times a set of coins of size _size_ 
with the given probability _prob_

```{r, eval = F}

# Get the density for 5 positive outcomes.
dbinom(5, 10, 0.5)

# For 0 positive outcomes.
dbinom(0, 10, 0.5)

# For 10 positive outcomes.
dbinom(10, 10, 0.5)
```

Plotting density distributions is not hard using `R`.
```{r, eval = F}
plot(dbinom(seq(0, 10, 1), 10, 0.5))
barplot(dbinom(seq(0, 10, 1), 10, 0.5))
plot(dbinom(seq(0, 10, 1), 10, 0.5), type = 'l')
```

Working distributions with different parameters..
```{r, eval = F}
plot(dbinom(seq(0, 100, 1), 100, 0.5), type = 'l', col = 'blue')
lines(dbinom(seq(0, 100, 1), 100, 0.6), col = 'red')
lines(dbinom(seq(0, 100, 1), 100, 0.4), col = 'green')
```

#### Distribution function.
```{r, eval = F}
p_of_x <- pbinom(seq(0, 10, 1), 10, 0.5)
p_of_x

plot(seq(0, 10, 1), p_of_x)
```

#### Binomial Tests

Binomial test is a very simple example of significance testing.

Let's start with the calculation of probabilities for special events. 
```{r, eval = F}
# Three heads.
dbinom(3, 3, 0.5)

# Probability of each event.
dbinom(0:3, 3, 0.5)

# Probability of two or tree heads.
sum(dbinom(2:3, 3, 0.5))

# Probability of getting 100 heads if we toss 100 times.
dbinom(100, 100, 0.5)
```

Now we can look on the the threashold values during the expriment.
```{r, eval = F}
# The probability to get more than 58 heads in such an experiment.
sum(dbinom(58:100, 100, 0.5))

# The probability to get more than 59 heads in such an experiment.
sum(dbinom(59:100, 100, 0.5))
```

For the most values we do need to calculate threasholds manually, we
can calculate the value of the quantile function.

```{r, eval = F}
qbinom(0.05, 100, 0.5, lower.tail=FALSE)
```

If we want to test the deviation without a clear directed hypothesis (more than or
less than) we can look at both sides.

```{r, eval = F}
sum(dbinom(c(0:40, 60:100), 100, 0.5))

sum(dbinom(c(0:39, 61:100), 100, 0.5))

qbinom(0.05/2, 100, 0.5, lower.tail=FALSE)

```

##### Testing with the normal distribution.
For the 1-tailed test with `p = 0.05`:
```{r, eval = F}
qnorm(0.05, lower.tail=TRUE)    # 1-tailed test, left panel of Figure 8
qnorm(1-0.95, lower.tail=TRUE)  # 1-tailed test, left panel of Figure 8
qnorm(0.05, lower.tail=FALSE)   # 1-tailed test, left panel of Figure 8
qnorm(1-0.95, lower.tail=FALSE) # 1-tailed test, left panel of Figure 8

qnorm(0.95, lower.tail=TRUE)    # 1-tailed test, right panel of Figure 8
qnorm(1-0.05, lower.tail=TRUE)  # 1-tailed test, right panel of Figure 8
qnorm(0.05, lower.tail=FALSE)   # 1-tailed test, right panel of Figure 8
qnorm(1-0.95, lower.tail=FALSE) # 1-tailed test, right panel of Figure 8
```

For the 2-tailed test with `p = 0.05`:
```{r, eval = F}
qnorm(0.025, lower.tail = TRUE)
qnorm(1-0.975, lower.tail = TRUE)
qnorm(0.975, lower.tail = FALSE)
qnorm(1-0.025, lower.tail = FALSE)

qnorm(0.975, lower.tail = TRUE)
qnorm(1-0.025, lower.tail = TRUE)
qnorm(0.025, lower.tail = FALSE)
qnorm(1-0.975, lower.tail = FALSE)

pnorm(-1.644854, lower.tail = TRUE)
pnorm(1.644854, lower.tail = FALSE)

2*pnorm(-1.959964, lower.tail = TRUE)
2*pnorm(1.959964, lower.tail = FALSE)
```


#### Plotting shaded areas
Confidence intervals could be represented graphically:

```{r, eval = F}
x1 <- seq(qnorm(0.95), qnorm(0.99999), length.out = 100)
y1 <- dnorm(x1)
qplot(qnorm(c(0.000001, 0.999999), lower.tail = T), stat = 'function', fun = dnorm, geom = 'line', xlab = 'Normal Curve') + geom_ribbon(mapping = aes(x = x1, ymax = y1, ymin = 0))
```

## <a name="fitting"></a>Fitting distributions and the Goodness of Fit

Many linguistic models are based on know distributions, e.g. binomial, Poisson, normal, Poya etc.

A usual usecase is to test if a sample is compatible with a supposed distribution.

Let's take the Normal distribution first:
```{r, eval = F}
set.seed(12345)
normal_data <- rnorm(1000)

shapiro.test(normal_data)


```

The Normal distribution is not often the case in linguistic data:
```{r, eval = F}
pause_times <- read.table('data/pauses.txt', header = T)
p_length <- pause_times$LENGTH
shapiro.test(p_length)
ks.test(p_length, 'pnorm', mean=mean(p_length), sd=sd(p_length))
```


Very useful could be some software for iterative fitting of empirical data sets.

## <a name="regression-models"></a>Linear Regression Models

The Presentation in this section are partly based on the `Chapter 2` from `Forte (2015)`
and `Chapter 5` from `Gries (2013)`.

$$
\hat y = \beta_0 + \beta_1 x 
$$

### Models based on artificial data

Let's try to prepare some artificial data set for the following model:
$$
y = 1.67x_1 - 2.93 + N(0, 2^2)
$$

```{r, cache = TRUE, eval = F}
#########################################
# Visual Depiction of Heteroskedasticity
#########################################

library(ggplot2)
set.seed(15)
x1 <- order(rnorm(200, mean = 100, sd = 25))
y <- sapply(x1, function(x) {17 + 1.8*x + rnorm(1, mean = 0, sd = x/4)})
df <- data.frame(x1 = x1, y = y)
fit<-coef(lm(y ~ x1, data = df))
p <- qplot(x1,y,data=df) + geom_abline(intercept=fit[1],slope=fit[2]) + ggtitle("Linear Relationship with Heteroskedastic Errors")+ 
     theme(plot.title = element_text(lineheight=.8, face="bold"))
p

#########################################
# Simple Linear Regression
#########################################

set.seed(5427395)
nObs = 100
x1minrange = 5
x1maxrange = 25
x1 = runif(nObs,x1minrange,x1maxrange)
e = rnorm(nObs,mean = 0, sd = 2.0)
y = 1.67*x1 - 2.93 + e
df = data.frame(y,x1)
myfit <- lm(y~x1,df)
summary(myfit)

p <- qplot(x1,y,data=df) 
p <- p + ggtitle("Simple Linear Regression")  
p <- p + geom_abline(intercept=coef(myfit)[1],slope=coef(myfit)[2], aes(linetype="Estimated Line"), size=1.2, show_guide=T)  
p <- p + geom_abline(intercept = -2.93, slope = 1.67, aes(linetype="Population Regression Line"),size=1.2, show_guide=T)  
p <- p + xlab("Feature x1")  
p <- p + ylab("Output y")  
p <- p + scale_linetype_manual(name="",values=c("Population Regression Line"=1,"Estimated Line"=2))  
p <- p + theme(plot.title = element_text(lineheight=.8, face="bold"),legend.position = "bottom")
p

#########################################
# Hardware Model Setup
#########################################

machine <- read.csv("data/machine.csv", header=F)
names(machine) <- c("VENDOR","MODEL","MYCT", "MMIN", "MMAX", "CACH", "CHMIN", "CHMAX", "PRP", "ERP")
machine <- machine[,3:9]
head(machine,n=3)

library(caret)
set.seed(4352345)
machine_sampling_vector <- createDataPartition(machine$PRP, p = 0.85, list = FALSE)
machine_train <- machine[machine_sampling_vector,]
machine_train_features <- machine[,1:6]
machine_train_labels <- machine$PRP[machine_sampling_vector]
machine_test <- machine[-machine_sampling_vector,]
machine_test_labels <- machine$PRP[-machine_sampling_vector]

machine_correlations <- cor(machine_train_features)
findCorrelation(machine_correlations)
findCorrelation(machine_correlations, cutoff = 0.75)
cor(machine_train$MMIN,machine_train$MMAX)

#########################################
# Cars Model Setup
#########################################

library(caret)
data(cars)

cars_cor <- cor(cars[,-1])
findCorrelation(cars_cor)
findCorrelation(cars_cor, cutoff=0.75)
cor(cars$Doors,cars$coupe)
table(cars$coupe,cars$Doors)

findLinearCombos(cars)

cars <- cars[,c(-15,-18)]
set.seed(232455)
cars_sampling_vector <- createDataPartition(cars$Price, p = 0.85, list = FALSE)
cars_train <- cars[cars_sampling_vector,]
cars_train_features <- cars[,-1]
cars_train_labels <- cars$Price[cars_sampling_vector]
cars_test <- cars[-cars_sampling_vector,]
cars_test_labels <- cars$Price[-cars_sampling_vector]

#########################################
# Constructing the Basic Models
#########################################

machine_model1 <- lm(PRP~.,data=machine_train)
cars_model1 <- lm(Price~.,data=cars_train)

summary(machine_model1)

#########################################
# Aliasing issue
#########################################

cars_model2 <- lm(Price~.-Saturn,data=cars_train)
summary(cars_model2)

#########################################
# Residuals
#########################################

machine_residuals <- machine_model1$residuals
machine_fitted_values <- machine_model1$fitted.values
machine_train_ids <- rownames(machine_train)
machine_large_residuals <- ifelse(abs(machine_residuals) > 150,machine_train_ids,'')
 
p1 <- qplot(machine_fitted_values,machine_residuals) 
p1 <- p1 + ggtitle("Residual Plot for CPU Data Set")
p1 <- p1 + theme(plot.title = element_text(lineheight=.8, face="bold"))
p1 <- p1 + xlab("Fitted Values")  
p1 <- p1 + ylab("Residuals")
p1 <- p1 + geom_text(size = 4, hjust=-0.15, vjust=0.1, aes(label=machine_large_residuals))
p1

cars_residuals <- cars_model1$residuals
cars_fitted_values <- cars_model1$fitted.values
cars_train_ids <- rownames(cars_train)
cars_large_residuals <- ifelse(abs(cars_residuals) > 9500,cars_train_ids,'')
 
p2 <- qplot(cars_fitted_values,cars_residuals) 
p2 <- p2 + ggtitle("Residual Plot for Cars Data Set") 
p2 <- p2 + theme(plot.title = element_text(lineheight=.8, face="bold")) 
p2 <- p2 + xlab("Fitted Values")  
p2 <- p2 + ylab("Residuals")
p2 <- p2 + geom_text(size = 4, hjust=-0.15, vjust=0.1, aes(label=cars_large_residuals))
p2

par(mfrow=c(2,1))
qqnorm(machine_residuals, main = "Normal Q-Q Plot for CPU data set")
qqline(machine_residuals)
qqnorm(cars_residuals, main = "Normal Q-Q Plot for Cars data set")
qqline(cars_residuals)

#########################################
# Significance Tests
#########################################

(q <- 5.210e-02 / 1.885e-02)
pt(q, df = 172, lower.tail = F) * 2

machine_model_null = lm(PRP~1,data=machine_train)
anova(machine_model_null, machine_model1)

n_machine <- nrow(machine_train)
k_machine <- length(machine_model1$coefficients) -1
sqrt(sum(machine_model1$residuals ^ 2) / (n_machine - k_machine - 1)) 

n_cars <- nrow(cars_train)
k_cars <- length(cars_model1$coefficients) -1
sqrt(sum(cars_model1$residuals ^ 2) / (n_cars - k_cars - 1))

mean(machine_train$PRP)
mean(cars_train$Price)

compute_rsquared <- function(x,y) {
  rss <- sum((x-y)^2)
  tss <- sum((y-mean(y))^2)
  return(1-(rss/tss))
}

compute_rsquared(machine_model1$fitted.values,machine_train$PRP)
compute_rsquared(cars_model2$fitted.values,cars_train$Price)

#########################################
# Comparing Regression Models
#########################################

compute_adjusted_rsquared <- function(x,y,p) {
  n <- length(y)
  r2 <- compute_rsquared(x,y)
  return(1 - ((1 - r2) * (n-1)/(n-p-1)))
}

compute_adjusted_rsquared(machine_model1$fitted.values,machine_train$PRP,k_machine)
compute_adjusted_rsquared(cars_model2$fitted.values,cars_train$Price,k_cars)

#########################################
# Test Set performance
#########################################

machine_model1_predictions <- predict(machine_model1, machine_test)
cars_model2_predictions <- predict(cars_model2, cars_test)

compute_mse <- function(predictions, actual) { mean((predictions-actual)^2) }

machine_model1_predictions <- predict(machine_model1, machine_test)
compute_mse(machine_model1$fitted.values, machine_train$PRP)
compute_mse(machine_model1_predictions, machine_test$PRP)

cars_model2_predictions <- predict(cars_model2, cars_test)
compute_mse(cars_model2$fitted.values, cars_train$Price)
compute_mse(cars_model2_predictions, cars_test$Price)

#########################################
# Variance Inflation Factor (VIF)
#########################################

library("car")
vif(cars_model2)

sedan_model <- lm(sedan ~.-Price-Saturn, data=cars_train)
sedan_r2 <- compute_rsquared(sedan_model$fitted.values,cars_train$sedan)
1 - (1-sedan_r2)

#########################################
# Outliers 
#########################################

machine_model2 <- lm(PRP~.,data=machine_train[!(rownames(machine_train)) %in% c(200),])
summary(machine_model2)

machine_model2_predictions <- predict(machine_model2, machine_test)
compute_mse(machine_model2_predictions, machine_test$PRP)

#########################################
# Feature selection
#########################################

machine_model3 <- step(machine_model_null, scope = list(lower = machine_model_null, upper=machine_model1), direction = "forward")

cars_model_null <- lm(Price~1,data=cars_train)
cars_model2 <- lm(Price~.-Saturn,data=cars_train)

cars_model_null <- lm(Price~1,data=cars_train)
cars_model3 <- step(cars_model2, scope=list(lower=cars_model_null, upper=cars_model2), direction="backward")

machine_model3_predictions <- predict(machine_model3, machine_test)
compute_mse(machine_model3_predictions, machine_test$PRP)

cars_model3_predictions <- predict(cars_model3, cars_test)
compute_mse(cars_model3_predictions, cars_test$Price)

#########################################
# Regularization
#########################################

library(glmnet)
cars_train_mat <- model.matrix(Price~.-Saturn, cars_train)[,-1]
lambdas <- 10 ^ seq(8,-4,length=250)
cars_models_ridge= glmnet(cars_train_mat,cars_train$Price,alpha=0,lambda=lambdas)
cars_models_lasso= glmnet(cars_train_mat,cars_train$Price,alpha=1,lambda=lambdas)

cars_models_ridge$lambdas[70]
coef(cars_models_ridge)[,70]

layout(matrix(c(1,2), 2, 1))
plot(cars_models_ridge, xvar = "lambda", main = "Coefficient Values vs. Log Lambda for Ridge Regression")
plot(cars_models_lasso, xvar = "lambda", main = "Coefficient Values vs. Log Lambda for Lasso")

layout(matrix(c(1,2), 1, 2))
plot(cars_models_ridge, xvar = "lambda", main = "Ridge Regression\n", col = gray.colors(1))
plot(cars_models_lasso, xvar = "lambda", main = "Lasso\n", col = gray.colors(1))

ridge.cv <- cv.glmnet(cars_train_mat,cars_train$Price,alpha=0,lambda=lambdas)
lambda_ridge <- ridge.cv$lambda.min
lambda_ridge

lasso.cv <- cv.glmnet(cars_train_mat,cars_train$Price,alpha=1,lambda=lambdas)
lambda_lasso <- lasso.cv$lambda.min
lambda_lasso

layout(matrix(c(1,2), 1, 2))
plot(ridge.cv, col = gray.colors(1))
title("Ridge Regression", line = +2)
plot(lasso.cv, col = gray.colors(1))
title("Lasso", line = +2)

predict(cars_models_lasso, type="coefficients", s = lambda_lasso)

cars_test_mat <- model.matrix(Price~.-Saturn, cars_test)[,-1]
cars_ridge_predictions <- predict(cars_models_ridge, s = lambda_ridge, newx = cars_test_mat)
compute_mse(cars_ridge_predictions, cars_test$Price)
cars_lasso_predictions <- predict(cars_models_lasso, s = lambda_lasso, newx = cars_test_mat)
compute_mse(cars_lasso_predictions, cars_test$Price)
```

## <a name="further-readings"></a>Where to go from here
If you were interested in the topics of the current presentation you may want to read ahead.
Some sources will help you to master Data Models and Predictive Analytics:

- Baayen, Rolf Harald. Analyzing Linguistic Data: A Practical Introduction to Statistics Using R. Cambridge University Press, 2008.
- Forte, Rui Miguel. Mastering Predictive Analytics with R: Master the Craft of Predictive Modeling by Developing Strategy, Intuition, and a Solid Foundation in Essential Concepts. Birmingham, Mumbai: Packt Publishing, 2015.
- Gries, Stefan Thomas. Statistics for Linguistics with R: A Practical Introduction. 2nd ed. Berlin: De Gruyter Mouton, 2013.

